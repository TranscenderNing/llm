{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3660ad9f-2d6e-4258-800d-1188865a43f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function current_device at 0x7f7b14268160>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "print(torch.cuda.current_device)\n",
    "\n",
    "\n",
    "train_data_path = '/home/public/ldn/zpLLM/data/train_data.json'\n",
    "eval_data_path = '/home/public/ldn/zpLLM/data/test_data.json'\n",
    "# Load dataset from the hub\n",
    "train_dataset = load_dataset(\"json\", data_files=train_data_path)\n",
    "eval_dataset = load_dataset(\"json\", data_files=eval_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70efa87-3b2c-48e6-a2f3-1b1f9951bb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['output', 'input', 'instruction'],\n",
       "         num_rows: 7384\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['output', 'input', 'instruction'],\n",
       "         num_rows: 1846\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset,eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d41bd7c-efe1-4035-9eb0-5c3a3a61b0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaiChuanTokenizer(name_or_path='/home/ldn/.cache/huggingface/hub/models--baichuan-inc--baichuan-7B/snapshots/c1a5c7d5b7f50ecc51bb0e08150a9f12e5656756', vocab_size=64000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "base_model = '/home/ldn/.cache/huggingface/hub/models--baichuan-inc--baichuan-7B/snapshots/c1a5c7d5b7f50ecc51bb0e08150a9f12e5656756'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model,trust_remote_code=True)\n",
    "tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-7B\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ce59514-bf19-45c5-a00f-3bee25a667b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaiChuanTokenizer(name_or_path='/home/ldn/.cache/huggingface/hub/models--baichuan-inc--baichuan-7B/snapshots/c1a5c7d5b7f50ecc51bb0e08150a9f12e5656756', vocab_size=64000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id = (\n",
    "        0  # unk. we want this to be different from the eos token\n",
    "    )\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7144565a-2148-4023-aa24-b40389b61e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'input', 'instruction'],\n",
       "        num_rows: 7384\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dce8c73-9bfd-4554-97a6-4d4a531b3b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# 格式化数据集格式\n",
    "def create_prompt_formats(sample):\n",
    "    INTRO_BLURB = \"以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['input']}\" if sample[\"input\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "    # print(parts)\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    # print(formatted_prompt)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "    return sample\n",
    "\n",
    "# tokenize a batch\n",
    "def preprocess_batch(batch,tokenizer,max_length):\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "# format and tokenize \n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    print('预处理数据集...')\n",
    "    # 每个样本添加提示\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "    print(dataset)\n",
    "    \n",
    "    # 去掉每个样本的无用的列\n",
    "    _preprocessing_function = partial(preprocess_batch,max_length=max_length,tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = [\"instruction\", \"input\", \"output\", \"text\"],\n",
    "    )\n",
    "    \n",
    "    # 过滤掉 input_ids 超出 max_length的样本\n",
    "    dataset = dataset.filter(lambda sample: len(sample['input_ids']) < max_length)\n",
    "    \n",
    "    # shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b30384b4-35bb-4fed-881c-65c2e8cd38e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理数据集...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['output', 'input', 'instruction', 'text'],\n",
      "        num_rows: 7384\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7275d29b56d8471cb97e52e1b8bbbd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad99d7a38be3447ea9aa85dc713f6358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 2048\n",
    "seed = 42\n",
    "train_dataset = preprocess_dataset(tokenizer,max_length,seed,train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d0f9d1a-e720-4558-9ec6-2a756c3e79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c64900ec-e368-4a76-b014-3fafbecd21d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 7384\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e895eb90-5b1c-4ac4-85a4-48f21a46409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\n",
      "\n",
      "### Instruction:\n",
      "判断一段话是否为诈骗话术，输出0或1，这段话为-->\n",
      "\n",
      "Input:\n",
      "哎你好老板我是前面那个骑手，去那个狂浪加油站，哎呀妈呀他是这样的他这个，呃时间有点不够你能给我加两分钟时间不呀，我说时间有点不够你能给我交两分钟时间不，噢你看你就是在那个上面b柱那个几楼几楼的上面你写写写写几个字，加上几个字就叫\n",
      "\n",
      "### Response:\n",
      "0\n",
      "\n",
      "### End\n",
      "以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\n",
      "\n",
      "### Instruction:\n",
      "判断一段话是否为诈骗话术，输出0或1，这段话为-->\n",
      "\n",
      "Input:\n",
      "啊喂吃了没，ah，咋没有说还没吃啊，找啥原因呢，那哈点个外卖吗没吃啊，完了那做了吗给你啊，没时间点名字快点点赶紧点点个面，我刚吃饱我看你贺叔张叔救的我刚吃完饭\n",
      "\n",
      "### Response:\n",
      "0\n",
      "\n",
      "### End\n",
      "以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\n",
      "\n",
      "### Instruction:\n",
      "判断一段话是否为诈骗话术，输出0或1，这段话为-->\n",
      "\n",
      "Input:\n",
      "你需要不需要需要需要，这里还还还有人啊，你看他比较细菌来我就是旁边有个人我不方便说我我说我，老师在吸血真厉害，给大家细节来，你的比较急什么\n",
      "\n",
      "### Response:\n",
      "0\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset[0]['input_ids']))\n",
    "print(tokenizer.decode(train_dataset[1]['input_ids']))\n",
    "print(tokenizer.decode(train_dataset[2]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6a6f11c-7cc2-4e4d-9143-a438f5b61350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ldn/anaconda3/envs/finetune/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ldn/anaconda3/envs/finetune/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/ldn/anaconda3/envs/finetune/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldn/anaconda3/envs/finetune/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/ldn/anaconda3/envs/finetune/lib/libcudart.so.11.0'), PosixPath('/home/ldn/anaconda3/envs/finetune/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-30 19:29:45,348] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                            device_map=\"auto\",         \n",
    "                                            load_in_8bit=True,\n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2eba503-1856-49ae-b307-e5a3041bc91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaiChuanForCausalLM(\n",
       "  (model): Model(\n",
       "    (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (W_pack): Linear8bitLt(in_features=4096, out_features=12288, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac358b3e-bb73-43e6-9ba3-29d736d50ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,777,216 || all params: 7,017,336,832 || trainable%: 0.23908238127452622\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    " r=32,\n",
    " lora_alpha=64,\n",
    " target_modules=[\"W_pack\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85d57d1e-0a09-4cf6-9ff4-ca2006b1e3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BaiChuanForCausalLM(\n",
       "      (model): Model(\n",
       "        (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (W_pack): Linear8bitLt(\n",
       "                in_features=4096, out_features=12288, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): RMSNorm()\n",
       "            (post_attention_layernorm): RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94951b81-6cb4-46b5-8c98-836b6ddda16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理数据集...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['output', 'input', 'instruction', 'text'],\n",
      "        num_rows: 1846\n",
      "    })\n",
      "})\n",
      "以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\n",
      "\n",
      "### Instruction:\n",
      "判断一段话是否为诈骗话术，输出0或1，这段话为-->\n",
      "\n",
      "Input:\n",
      "我是中国人民武装部队天水部队，要求订餐，加微信\n",
      "\n",
      "### Response:\n",
      "1\n",
      "\n",
      "### End\n",
      "以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\n",
      "\n",
      "### Instruction:\n",
      "判断一段话是否为诈骗话术，输出0或1，这段话为-->\n",
      "\n",
      "Input:\n",
      "我是淘宝客服，为了回馈来客户要给你送一个杯子。你杯子收到货没有，你给我一个好评，并且会给你好处费15块，你添加一个企业微信\n",
      "\n",
      "### Response:\n",
      "1\n",
      "\n",
      "### End\n",
      "以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\n",
      "\n",
      "### Instruction:\n",
      "判断一段话是否为诈骗话术，输出0或1，这段话为-->\n",
      "\n",
      "Input:\n",
      "哦，day，they，我我听着喂，我最好，我才说火山的\n",
      "\n",
      "### Response:\n",
      "0\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "max_length = 2048\n",
    "seed = 42\n",
    "test_dataset = preprocess_dataset(tokenizer,max_length,seed,eval_dataset)\n",
    "test_dataset = test_dataset['train']\n",
    "print(tokenizer.decode(test_dataset[1]['input_ids']))\n",
    "print(tokenizer.decode(test_dataset[2]['input_ids']))\n",
    "print(tokenizer.decode(test_dataset[3]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9d1fbca-c8ce-4178-b793-42e19b60b9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtranscenderning\u001b[0m (\u001b[33mtranscender\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90366579d8664284be1776bc1f74447b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668649450002702, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/public/ldn/zpLLM/wandb/run-20230830_194541-661isrtl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/transcender/huggingface/runs/661isrtl' target=\"_blank\">sage-salad-34</a></strong> to <a href='https://wandb.ai/transcender/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/transcender/huggingface' target=\"_blank\">https://wandb.ai/transcender/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/transcender/huggingface/runs/661isrtl' target=\"_blank\">https://wandb.ai/transcender/huggingface/runs/661isrtl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldn/anaconda3/envs/finetune/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1846' max='1846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1846/1846 5:03:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.464600</td>\n",
       "      <td>1.697930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.732000</td>\n",
       "      <td>1.617050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.436500</td>\n",
       "      <td>1.573681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.668600</td>\n",
       "      <td>1.546007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.591500</td>\n",
       "      <td>1.520054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.459000</td>\n",
       "      <td>1.502976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.339400</td>\n",
       "      <td>1.489926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.397000</td>\n",
       "      <td>1.477730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.409300</td>\n",
       "      <td>1.471586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldn/anaconda3/envs/finetune/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/ldn/anaconda3/envs/finetune/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/ldn/anaconda3/envs/finetune/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/ldn/anaconda3/envs/finetune/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import transformers\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_torch\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_steps=400,\n",
    "        output_dir='/home/public/ldn/models/zp-baichuan7b-lora',\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "trainer.train()\n",
    "new_model = \"/home/public/ldn/models/zp-baichuan7b-lora/zp-lora\"\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfc418c7-6d2b-4d80-9542-aa8006505242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[31106,  4567, 31161, 14277,  1197,  5934, 31135, 17344,    72, 31488,\n",
       "         31628, 31827,  1197, 11886, 31135,  8950,    72,  3750, 31577,  5934,\n",
       "            73,     5,     5, 26411,  2081,  2768, 31143,     5, 10345, 14055,\n",
       "         31455,  4531, 31178, 10910, 31455, 31538,    72, 17515,    52, 31399,\n",
       "            53,    72, 20480, 31455, 31178,    90,     5,     5, 16401, 31143,\n",
       "             5, 31203, 15380, 31378,  4579, 32617, 32889, 17060, 31515, 17740,\n",
       "         31205,    72, 34065, 34065, 34065, 31203, 31203,  6249, 31682, 32021,\n",
       "            72,  6646, 32617, 31382,  6646,  1522, 10320,  1925, 31382,  6646,\n",
       "         31505, 31435,  9928,  1925,  9344,  9402, 31135, 31763,    72, 31379,\n",
       "          4054,  9344, 10405, 31523, 12127, 31135,    72, 31357, 32613, 31473,\n",
       "         11330, 31350, 35331, 31247, 31172,     5,     5, 26411, 16275, 31143]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = '以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\\n\\n### Instruction:\\n判断一段话是否为诈骗话术，输出0或1，这段话为-->\\n\\nInput:\\n你记住管他的啥呀话说完能不能行，嗯嗯嗯你你过去买哈，那个啥从那个就是原来不是从那个北门出去不是有一个广场的吗，那地方有一个一家清超市的，外婆头有个手擀面了\\n\\n### Response:'\n",
    "model_input = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9bbb1b2-2592-4ade-a7eb-9cba461f7767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\\n\\n### Instruction:\\n判断一段话是否为诈骗话术，输出0或1，这段话为-->\\n\\nInput:\\n你记住管他的啥呀话说完能不能行，嗯嗯嗯你你过去买哈，那个啥从那个就是原来不是从那个北门出去不是有一个广场的吗，那地方有一个一家清超市的，外婆头有个手擀面了\\n\\n### Response:\\n0\\n\\n### End:\\n嗯嗯'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(**model_input, max_new_tokens=10)[0]\n",
    "output\n",
    "output = tokenizer.decode(output,skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1dbecc9-aaba-45b3-8875-16be6e8bc571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[output.find('Response')+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4fcf903-aa91-48c9-9135-48bf1d351840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "class ZpData(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        with open(data_file, mode='r') as f:\n",
    "            data = f.read()\n",
    "        samples = json.loads(data)\n",
    "        \n",
    "        Data = {}\n",
    "        for idx, sample in enumerate(samples):\n",
    "            formatted_prompt = self.create_prompt_formats(sample)\n",
    "            Data[idx] = {'text': formatted_prompt, 'label': sample['output']}\n",
    "        \n",
    "        return Data\n",
    "\n",
    "    def create_prompt_formats(self, sample):\n",
    "        INTRO_BLURB = \"以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\"\n",
    "        INSTRUCTION_KEY = \"### Instruction:\"\n",
    "        INPUT_KEY = \"Input:\"\n",
    "        RESPONSE_KEY = \"### Response:\"\n",
    "        \n",
    "        blurb = f\"{INTRO_BLURB}\"\n",
    "        instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "        input_context = f\"{INPUT_KEY}\\n{sample['input']}\" if sample[\"input\"] else None\n",
    "        response = f\"{RESPONSE_KEY}\"\n",
    "        parts = [part for part in [blurb, instruction, input_context, response] if part]\n",
    "        formatted_prompt = \"\\n\\n\".join(parts)\n",
    "        return formatted_prompt\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1083c51-3c84-4e90-96dd-2d04db430b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\\n\\n### Instruction:\\n判断一段话是否为诈骗话术，输出0或1，这段话为-->\\n\\nInput:\\n你记住管他的啥呀话说完能不能行，嗯嗯嗯你你过去买哈，那个啥从那个就是原来不是从那个北门出去不是有一个广场的吗，那地方有一个一家清超市的，外婆头有个手擀面了\\n\\n### Response:', 'label': '0'} {'text': '以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\\n\\n### Instruction:\\n判断一段话是否为诈骗话术，输出0或1，这段话为-->\\n\\nInput:\\n平安银行贷款部的工作人员，有一至五万的额度可以申请，详细情况稍后会有工作人员添加你的微信给你解答\\n\\n### Response:', 'label': '1'} {'text': '以下是描述一个任务的指示，请编写一个适当的回答，完成该任务。\\n\\n### Instruction:\\n判断一段话是否为诈骗话术，输出0或1，这段话为-->\\n\\nInput:\\n抖音，我是你的小可爱，这样好为了支持中小微企业的发展京东金融特地为此类中小微企业主提供最高五十万的贷款服务您要是正好有这方面需求的话我给您介绍一下好吗，这个贷款产品呢最高有五十万的额度现在推广期内我们会给本次电话通知到的业主免费用十次天的福利这个真的是机会难得以后\\n\\n### Response:', 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "test_data = ZpData('/home/public/ldn/zpLLM/data/test_data.json')\n",
    "print(test_data[0],test_data[1],test_data[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6efc8435-f8ef-4be1-a2de-aebf4d919925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# batch\n",
    "def collote_fn(batch_samples):\n",
    "    batch_sentence_1 = []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_sentence_1.append(sample['text'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    X = tokenizer(\n",
    "        batch_sentence_1, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a273f2f1-e04f-4b20-b835-4566479e148c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1846"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True, collate_fn=collote_fn)\n",
    "size = len(test_dataloader.dataset)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f008653d-51c4-4239-9330-bdb0c28cf80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea66a3e5-11d1-442d-bb86-838d18bd8a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------batch1----------------------------------\n",
      "\n",
      "-------------------------------------shape----------------------------------\n",
      "\n",
      "\n",
      "batch_X shape: {'input_ids': torch.Size([1, 69]), 'attention_mask': torch.Size([1, 69])}\n",
      "\n",
      "\n",
      "batch_y shape: torch.Size([1])\n",
      "\n",
      "\n",
      "\n",
      "推理时间为：11.778877019882202 秒\n",
      "\n",
      "\n",
      "----------------------------------final result---------------------------------\n",
      "\n",
      "[1]\n",
      "\n",
      "\n",
      "----------------------------------predictions---------------------------------\n",
      "\n",
      "tensor([1], device='cuda:0')\n",
      "\n",
      "\n",
      "----------------------------------labels---------------------------------\n",
      "\n",
      "tensor([1], device='cuda:0')\n",
      "\n",
      "\n",
      "----------------------------------correct---------------------------------\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "----------------------------------total correct---------------------------------\n",
      "\n",
      "1\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldn/anaconda3/envs/finetune/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "device = \"cuda\"\n",
    "import time\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for X, y in test_dataloader:\n",
    "        print(f'\\n-------------------------------------batch{i+1}----------------------------------\\n')\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(f'-------------------------------------shape----------------------------------\\n')\n",
    "        print('\\nbatch_X shape:', {k: v.shape for k, v in X.items()})\n",
    "        print(\"\\n\")\n",
    "        print('batch_y shape:', y.shape)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        output = model.generate(**X, max_new_tokens=10)\n",
    "        output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"\\n推理时间为：{execution_time} 秒\\n\")\n",
    "\n",
    "        result = []\n",
    "        for str in output:\n",
    "            result.append(int(str[str.find('Response')+10]))\n",
    "        print(\"\\n----------------------------------final result---------------------------------\\n\")\n",
    "        print(f\"{result}\\n\")\n",
    "        preds = torch.tensor(result, device=device)\n",
    "        print(\"\\n----------------------------------predictions---------------------------------\\n\")\n",
    "        print(f\"{preds}\\n\")\n",
    "        print(\"\\n----------------------------------labels---------------------------------\\n\")\n",
    "        print(f\"{y}\\n\")\n",
    "        print(\"\\n----------------------------------correct---------------------------------\\n\")\n",
    "        print(f\"{(preds == y).sum().item()}\\n\")\n",
    "        correct += (preds == y).sum().item()\n",
    "        print(\"\\n----------------------------------total correct---------------------------------\\n\")\n",
    "        print(correct)\n",
    "        i += 1\n",
    "        import sys\n",
    "        sys.exit(0)\n",
    "        \n",
    "correct /= size\n",
    "print(f\"{mode} Accuracy: {(100*correct):>0.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb2ae3-019b-45c8-95ae-63f31f86e750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
